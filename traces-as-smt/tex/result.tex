\section{Experiments and Results}
%In Section 4, we know that a well defined SMT problem can encode all possible execution traces for a CTP given a precise set of match pair. A precise set of match pair contains all possible match pairs that can exist in the real execution, and all match pairs that can not exist in the execution are not included. In other words, every hidden error can be found if it exists in some trace.
We used the MCA provided reference solution to generate MCAPI program traces. The reference solution uses the \textit{PThread} library to create multiple MCAPI tasks. Our tool takes the trace as input, computes the match-pair set, and outputs the trace encoding. We use the \textit{Yices} SMT solver \cite{dutertre:CAV06} to check the satisfiability of the generated SMT encoding.

We use \textit{zero-buffer} semantics and compare our results to that in \cite{elwakil:padtad10}.  Two sets of benchmarks are used in the comparison. The first set consists of four ``toy" examples, including the example in \figref{fig:mcapi}. The other three benchmarks are generated manually. \textit{Small1} is a program execution with two tasks, where two sends are in the first task, and two receives are in the second task.  \textit{Small2} is a program execution with only one task, where one send and one receive are contained.  \textit{Small3} is a program execution with three tasks, where three sends are in the first task, two receives are in the second task, and one receive is in the third task. Those examples are very short execution traces. The other set of benchmarks consists of five large program traces. One of them, we call ``leader election", basically elects a leader from several candidates by message passing. The other four program traces, are all extensions to the one in \figref{fig:mcapi}. In particular, extra iterations are added to the original program execution to generate longer execution traces. In all benchmarks, the correctness properties are numerical assertions over variables. The experiments are conducted on a PC with a 1.6 GHz Intel Quad Core processor and 8GB memory running Ubuntu 14.

\begin{table}
\begin{center}
\scriptsize
\begin{tabular}{|c|c|c|c|c|c|}
    \hline
     &Program Order&Matches&Assume$\&$Assert&Extra Clauses\\
    \hline
    Our Encoding & 11 & 4 & 2 & 0\\
    Encoding in \cite{elwakil:padtad10} & 22 & 13 & 3 & 8\\
    \hline
\end{tabular}
\end{center}
\caption{Comparison of two encodings for the MCAPI program execution in \figref{fig:mcapi}.}
\label{table:program}
\end{table}
As for the first set of benchmarks, \tableref{table:program} shows the comparison of our encoding and the encoding in \cite{elwakil:padtad10} on the example in \figref{fig:mcapi}.  Basically, we encode 17 clauses for modeling the program execution in \figref{fig:mcapi}, omitting the definition of variables at the beginning of the encoding. Instead, the encoding in \cite{elwakil:padtad10} has 47 clauses for the same program execution, omitting the definitions as well. The number of clauses that constrain program order, match pairs, assume/asserts, and any other extra uncategorized clauses are also shown in \tableref{table:program}. Program order comprises the bulk of the encoding. The extra clauses in \cite{elwakil:padtad10} come from auxiliary variables in the encoding.

\begin{table}
\begin{center}
\scriptsize
\begin{tabular}{|l|c||c|c|c|c|}
		\hline
         \multicolumn{2}{|c||}{Test Programs} & \multicolumn{2}{|c|}{Our Encoding} & \multicolumn{2}{|c|}{Encoding in \cite{elwakil:padtad10}}\\ \hline
         Name & \# Messages & Property & \# Clauses & Property & \# Clauses \\ \hline
         EP in \figref{fig:mcapi} & 3 & sat & 17 & unsat & 47 \\
         \textit{Small1} & 2 & unsat & 8 & unsat & 33 \\
         \textit{Small2} & 1 & unsat  & 4 & unsat & 18 \\
         \textit{Small3} & 3 & sat & 11 & unsat & 44 \\
         \hline
		\end{tabular}
\end{center}
\caption{Comparison of two encodings for four ``toy" MCAPI program executions with property and clause number.}
\label{table:comparison}
\end{table}

\tableref{table:comparison} shows the properties of our encodings and the encodings in \cite{elwakil:padtad10} on each benchmark.  The SMT solver returns the correct answer of our encoding under \textit{zero-buffer} semantics for each example shown in \tableref{table:comparison}. In particular, our encodings return ``sat" (a satisfiable solution) for the example in \figref{fig:mcapi} and \textit{Small3} since violations are detected for both scenarios. Also, our encodings return ``unsat" (an unsatisfiable solution) for \textit{Small1} and \textit{Small2}, meaning there are no errors in program runtimes. From the results, it is also shown that the encoding in \cite{elwakil:padtad10} does not always encode the correct program behavior. In addition to the correctness properties, the number of clauses shown in \tableref{table:comparison} follows the performance of the SMT solver with fewer clauses leading to better runtime. \tableref{table:comparison} shows that our encoding generates  1/5 -- 1/3  the clauses of that of the encoding in \cite{elwakil:padtad10}. We believe these reductions will generalize to any given program trace and match pair set.

\begin{table}
\begin{center}
\scriptsize
\begin{tabular}{|l|c||c|c|c|c|}
		\hline
         \multicolumn{2}{|c||}{Test Programs} & \multicolumn{2}{|c|}{Our Encoding} & \multicolumn{2}{|c|}{Encoding in \cite{elwakil:padtad10}}\\ \hline
         Name & \# Messages & Time(ms) & Memory(MB) & Time(ms) & Memory(MB) \\ \hline
         \textit{LeaderElect} & 30 & -- & -- &-- &-- \\
         \textit{5iterations} & 15 & 72 &  18.5195 & 392 & 37.2188 \\
         \textit{6iterations} & 18 & 72  & 18.918 & 636 & 48.7031 \\
         \textit{7iterations} & 21 & 80 & 19.375 & 940 & 62.7188 \\
         \textit{8iterations} & 24 & -- &-- &-- &-- \\
         \hline
		\end{tabular}
\end{center}
\caption{Comparison of two encodings for five ``large" MCAPI program executions with runtime and used memory.}
\label{table:comparison1}
\end{table}

As for the second set of benchmarks, we also use \textit{zero-buffer} semantics to match \cite{elwakil:padtad10}. The runtime and memory usage for each program execution in \tableref{table:comparison1} are compared. In this series of experiments, the encodings in \cite{elwakil:padtad10} are not capable of resolving all the non-deterministic behaviors without \textit{infinite-buffer} semantics. As such, we revise our encodings for the same scenarios in \textit{zero-buffer} setting in order to obtain the same behaviors of the encodings in \cite{elwakil:padtad10}. \tableref{table:comparison1} shows the experimental results for the ``large" MCAPI program executions. The runtimes and the memory usage of our encodings are smaller than those of the encodings in \cite{elwakil:padtad10} for all benchmarks in \tableref{table:comparison1}. In average, our encodings run eight times faster than the encodings in \cite{elwakil:padtad10} and use two times less memory than the encodings in \cite{elwakil:padtad10}.

By comparing to the encoding in \cite{elwakil:padtad10} on two series of experiments shown above, our encoding is demonstrated that it can correctly encode the non-determinism of an MCAPI program execution, and can be executed efficiently.

