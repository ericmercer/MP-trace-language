\section{Experiments and Results}
To assess the new encoding in this paper, three experiments with results are presented: a comparison to prior SMT encodings on a zero-buffer semantics, a scalability study on the effects of non-determinism in the execution time on infinite buffer semantics, and an evaluation on typical benchmark programs again with infinite buffer semantics. All of the experiments use the Z3 SMT solver (\cite{demoura:tacas08}) and are measured on a 2.40 GHz Intel Quad Core processor with 8 GB memory running Ubuntu 14.

The initial program trace for the experiments is generated using MCA provided reference solution with fixed input to isolate all non-determinism to how sends and receives are matched in the runtime. This statement means that the experiments only consider one path of control flow through the program. Complete coverage of the program for verification purposes would need to generate input to exercise different control flow paths.  Where appropriate, the time to generate the match-pair sets from the input trace is reported separately. 

\subsection{Comparison to Prior SMT Encoding}
To the best knowledge of these authors, the current most effective SMT encoding for verification of message passing program traces is an order-based encoding that captures the happens-before relation directly in the SMT problem \cite{elwakil:padtad10}. The encoding only exists for zero-buffer semantics, and the tool to generate the encoding is not publicly available. The authors of the order-based encoding, however, graciously encoded several contrived benchmarks used for correctness testing. These benchmarks are best understood as \emph{toy} examples that plumb the MCAPI semantics to clarify intuition on expected behavior.

The zero-buffer encoding in this paper is compared directly to the order-based encoding on the contrived benchmarks. The order-based encoding yields incorrect answers for several programs. Where the order-based encoding returns correct answers, the new encoding, on average, requires 70\% fewer clauses, uses half the memory as reported by the SMT solver, and runs eight times faster. The dramatic improvement of the new encoding over the order-based encoding is a direct result of the match-pairs that simplify the happens-before constraints and avoids redundant constraints in the transitive closure of the happens-before relation.

\subsection{Scalability Study}

The intent of the scalability study is to understand how performance is affected by the number of messages in the program trace and the level of non-determinism in choosing match-pairs where multiple sends are able to match to multiple receives. The program for this study consists of 51 threads. The first thread receives 50 messages containing integer values and then asserts that every message did not receive a specific value. In other words, a violation is one where each message has a specific value.  The remaining 50 threads send a message, each containing a different unique integer value, to the first thread. This program represents the worst-case scenario for non-determinism in a message passing program as any send is able to match with any receive in the runtime and the assertion is only violated when each send is paired with a specific receive. The SMT solver must search through the multitude of match-pairs to the single precise subset of match-pairs that trigger the violation.

The study takes this initial program of 51 threads and varies the number of match pairs used in the encoding to reduce the non-determinism that must be resolved by the SMT solver so the program can be made hard (i.e., a lot of possibilities) or easy (i.e., a few possibilities). For example, the worst-case is when there are 2500 match pairs (50 sends times 50 receives). In this scenario, there are many feasible subsets of match-pairs that match every receive to a unique send, one of which violates the assertion. Suppose now that the original set of 2500 match pairs is reduced to only 100 match pairs, and of these 100 match-pairs there are exactly two subsets that are feasible, one of which violates the assertion. In this way, it is possible to vary the number of match-pair subsets that represent a feasible pairing of every send and receive making it harder or easier for the SMT solver to find the violating subset. All 2500 match-pairs is a \emph{hard} problem while \emph{100} match-pairs is an easy problem.


\begin{table}
\begin{center}
\scriptsize
\caption{Scaling as a function of non-determinism}
\begin{tabular}{|c|c|c|c|}
		\hline
         \multicolumn{2}{|c|}{Test Programs} & \multicolumn{2}{|c|}{Performance} \\ \hline
          Match Pairs & Feasible Sets &  Time (hh:mm:ss) & Memory(MB) \\ \hline
100 &          2      &    $<$00:00:01    &     4.94 \\
500 &       $\sim$4E5 &    $<$00:00:01    &     7.95 \\
800 &       $\sim$5E8 &    $<$00:00:01    &        10.11 \\
850 &       $\sim$2E13 &    $<$00:00:01   &      10.43 \\
900 &       $\sim$3E14 &    $<$00:00:01   &      10.70 \\
1000 &      $\sim$2E18  &   00:00:35       &     31.07 \\
1400 &      $\sim$3E29  &   00:02:12       &     51.48 \\
1600 &     $\sim$3E35   &  00:03:24        &    62.32 \\
2000 &     $\sim$8E47   &  00:07:00        &    80.35 \\
2500 &     $\sim$3E64   &  00:12:23        &    91.97 \\
         \hline
		\end{tabular}
\end{center}
\label{table:first}
\end{table}

\tableref{table:first} shows how the new encoding scales with hardness. The first column is the number of potential match-pairs and the second column is the number of feasible match-pair subsets that correctly match every receive to a unique send. As expected, running time and memory consumption increase with hardness.

\begin{table}
\begin{center}
\setlength{\tabcolsep}{2pt}
\scriptsize
\caption{Performance on selected benchmarks}
\begin{tabular}{|l|c|c|c|c|c|c|}
		\hline
         \multicolumn{3}{|c|}{Test Programs} & \multicolumn{4}{|c|}{Performance} \\ \hline
         Name & \# Mesg & Feasible Sets & EG(s) & MG(s) & Time (hh:mm:ss) & Memory(MB) \\ \hline
         \textit{LE} & 620 & 1 & 1.49 & 0.051 & $<$00:00:01 & 33.41  \\ % MCAPI benchmarks we made
         \textit{Router} & 200 & $\sim$6E2 & 0.417 & 0.032 & 00:00:02 & 15.03  \\ % MCAPI benchmark from our lab
         \textit{MultiM} & 100 & $\sim$1E40 & 0.632 & 0.436 &  00:16:40 & 135.19  \\ % MCAPI benchmark from our lab
         \textit{Pktuse} & 512 & $\sim$1E81 & 10.190 & 9.088 & 02:06:09 & 1539.90 \\ % MPI benchmark converted to MCAPI [2] (others point to point or barriered)
         \hline
		\end{tabular}
\end{center}
\label{table:second}
\end{table}

\subsection{Typical Benchmark Programs}
The goal of these experiments is to measure the new encoding on several benchmark programs. MCAPI is a new interface, and to date, the authors are not aware of publicly available programs written against the interface aside from the few toy programs that come with the library distribution. As such, the benchmarks in the experiments come from a variety of sources. The first program, \textit{LE}, is the leader election problem and is common to benchmarking verification algorithms. The second program, \textit{Router}, is an algorithm to update routing tables. Each router node is in a ring and communicates only with immediate neighbors to update the tables. The program ends when all the routing tables are updated. The third program, \textit{MultiM}, is an extension to an program in MCAPI library distribution \figref{fig:mcapi1}. The extension adds extra iterations to the original program execution to generate longer execution trace. The last program in the benchmark set, \textit{Pktuse}, and benchmark from the MPI test suite \cite{mpptest_benchmark}. The program creates 5 tasks---each of which randomly sends several messages to the other tasks.

The benchmark programs are intended to cover a spectrum of program properties. As before, the primary measure of hardness in the programs in not the number of messages but rather the size of the match-pair set and the number of feasible subsets.  The \textit{LE} program is the easiest program in the suite. Although it sends 620 messages, there is only a single feasible match-pair set. The programs \textit{Router}, \textit{MultiM}, and \textit{Pktuse} respectively increase in hardness, which again is not related to the total number of messages but rather the total number of feasible match-sets that must be considered. For example, even though \textit{Router} has 200 messages, it is an easier problem that \textit{MultiM} that has 100 messages. The \textit{Pktuse} program does have the most number of messages, 512, and in this case, the largest number of feasible match-pair sets.

\tableref{table:second} shows the results for the benchmark suite. Other than the metrics used in \tableref{table:first}, the time of generating the encoding and the match pairs is included in the third and fourth columns respectively. Note that the time shown in the third column includes the time in the fourth column. As before, the running time tracks hardness and not the total number of messages. The table also shows the cost of match-pair generation as it dominates the encoding time for the \textit{Pktuse} program. Future work is to address the high-cost of match-pair generation, which the authors believe to be NP-complete. The proof is part of the suggested future work.

The benchmark suite demonstrates that a message passing program may have a large degree of non-determinism in the run time that is prohibitive to verification approaches that directly enumerate that non-determinism such as a model checker. The SMT encoding, however, pushes the problem to the SMT solver by generating the possible match-pairs and then relying on advances in SMT technology to resolve the non-determinism in a way that violates the assertion. Of course, the SMT problem itself is NP-complete, so performance is only reasonable for small problem instances. The benchmark suite suggests that problem instances with astonishingly large numbers of feasible match pair sets are able to complete in a reasonable amount of time using the new encoding in this paper; though, the time to generate the match-pairs may quickly become prohibitive.

